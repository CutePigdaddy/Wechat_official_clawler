import requests
import json
from bs4 import BeautifulSoup
import re
import json
import os
from playwright.sync_api import sync_playwright

#HEADERSéœ€è¦æŠ“åŒ…è·å–
s = requests.Session()
headers = {}
default_headers = {
  "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 NetType/WIFI MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63090a13) UnifiedPCWindowsWechat(0xf2541022) XWEB/16467 Flue",
  "Referer":"https://mp.weixin.qq.com/mp/"
}
def form_headers(headers_text):
  """
  åŠŸèƒ½ï¼šè¾“å…¥textç‰ˆæœ¬çš„headersè‡ªåŠ¨è½¬åŒ–ä¸ºå­—å…¸headers
  :param headers_text:åœ¨Fiddleré‡Œé¢æ‰¾åˆ°mp.weixin.qq.comåé¢å¸¦æœ‰getmsgï¼ˆæ²¡æœ‰çš„è¯åœ¨ä¸»é¡µå¾€ä¸‹æ‹‰è®©ä»–åˆ·æ–°ï¼‰çš„è¯·æ±‚ï¼Œæ‰¾åˆ°å³è¾¹çš„headersï¼Œå³é”®å¤åˆ¶æ‰€æœ‰headersï¼ŒæŠŠå‰é¢ä¸€æ®µGET...ä¸€å¤§æ®µURLåˆ æ‰ï¼Œä¿ç•™ä¸‹é¢çš„headers
  :return:è¿”å›ä¸€ä¸ªheaders Dictæ•°æ®
  ä¾‹å¦‚ï¼š
        Host: mp.weixin.qq.com
        Connection: keep-alive
        User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 NetType/WIFI        MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63090a13) UnifiedPCWindowsWechat(0xf2541022) XWEB/16467 Flue
        X-Requested-With: XMLHttpRequest
        Accept: */*
        Sec-Fetch-Site: same-origin
        Sec-Fetch-Mode: cors
        Sec-Fetch-Dest: empty
        Referer: https://mp.weixin.qq.com/mp/profile_ext?....
        Accept-Encoding: gzip, deflate, br
        Accept-Language: zh-CN,zh;q=0.9
        Cookie: ......

  """
  for line in headers_text.splitlines():
      line = line.strip()
      if not line or ":" not in line:
          continue  # è·³è¿‡ç©ºè¡Œæˆ–æ— å†’å·è¡Œ
      key, value = line.split(":", 1)
      headers[key.strip()] = value.strip()
  s.headers.update(headers)
  print("Headersé…ç½®æˆåŠŸï¼")
  return headers

def fetch_accounts(article_url):
  """
  åŠŸèƒ½ï¼šé€šè¿‡æŸä¸ªæ–‡ç« çŸ­é“¾è·å–å…¬ä¼—å·ä¸»é¡µçš„URL, å¹¶è¾“å‡ºæç¤º
  :param article_url:å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çš„çŸ­é“¾ï¼Œä¸€å®šå¾—æ˜¯çŸ­é“¾å“¦
  """
  print(headers)
  document = fetch_article_details(article_url,10)
  account_url = f"https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz={document["biz"]}&scene=124#wechat_redirect"
  print(f"æˆåŠŸè·å–å…¬ä¼—å·ä¸»é¡µURLâ†’{account_url}")
  print("å¯ä»¥å¤åˆ¶åˆ°å¾®ä¿¡æ–‡ä»¶ä¼ è¾“åŠ©æ‰‹ï¼Œç„¶åé€šè¿‡å¾®ä¿¡å®¢æˆ·ç«¯æ‰“å¼€ï¼")

def fetch_article_details(url,timeout):
  """
  åŠŸèƒ½ï¼šè¾“å…¥å…¬ä¼—å·æ–‡ç« URLï¼Œå¯ä»¥è·å–æ–‡ç« ä½œè€…ã€æ ‡é¢˜ç­‰
  :param url: å…¬ä¼—å·æ–‡ç« url
  :param timeout: è·å–é—´éš”
  :return: Dictæ•°æ®ï¼Œæœ‰"status","content","title","author","create_time","biz"ï¼Œstatusä¸º1çš„æ—¶å€™æˆåŠŸï¼Œä¸º0çš„æ—¶å€™å¤±è´¥
  """
  url = url.strip() #å»é™¤å¤´å°¾ç©ºæ ¼æ¢è¡Œ

  print("-----å¼€å§‹è¯·æ±‚-----")
  resp = s.get(url,headers=default_headers,timeout=timeout,allow_redirects=True,verify=False)
  if resp.status_code == 200:
    print("âˆšâˆšâˆšâˆšâˆšè¯·æ±‚æˆåŠŸï¼âˆšâˆšâˆšâˆšâˆš")
  else:
    print("Ã—Ã—Ã—Ã—Ã—è¯·æ±‚å¤±è´¥ğŸ¥¹Ã—Ã—Ã—Ã—Ã—")
    return {"status":0}
  resp.encoding = resp.apparent_encoding
  status = re.search("å½“å‰ç¯å¢ƒå¼‚å¸¸ï¼Œå®ŒæˆéªŒè¯åå³å¯ç»§ç»­è®¿é—®",resp.text)
  if status:
    print("!!!!!ç¯å¢ƒå¼‚å¸¸,ç¨‹åºæ‰§è¡Œå¤±è´¥!!!!!!")
    return {}
  html = resp.text
  soup = BeautifulSoup(html,"lxml")
  print("-----å¼€å§‹æœç´¢å…ƒç´ -----")
  content = soup.find("div",class_ = "rich_media_content").get_text("\n",strip=True)
  if content:
    print("æˆåŠŸæ‰¾åˆ°æ–‡ç« å†…å®¹ï¼")
    print(content[:10]+"...")
  title = soup.find("h1",{"class":"rich_media_title","id":"activity-name"}).get_text(strip=True)
  if title:
    print("æˆåŠŸæ‰¾åˆ°æ–‡ç« æ ‡é¢˜ï¼")
    print(f"[Title]â†’{title}")
  author = soup.find("a",{"id":"js_name"}).get_text(strip=True)
  if author:
    print("æˆåŠŸæ‰¾åˆ°æ–‡ç« ä½œè€…ï¼")
    print(f"[Author]â†’{author}")
  biz = re.search(r'var biz\s*=\s*"(.*?)";',html).group(1).replace('" || "','').replace('"','')
  if biz:
    print("æˆåŠŸè·å–å…¬ä¼—å·é“¾æ¥ï¼")
    print(f"[Official]â†’https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz={biz}&scene=124#wechat_redirect")
  create_time = re.search(r"var createTime = '(.*?)';",html).group(1)
  if create_time:
    print("æˆåŠŸè·å–æ–‡ç« åˆ›å»ºæ—¶é—´ï¼")
    print(f"CreateTimeâ†’{create_time}")
  os.makedirs("HTML",exist_ok=True)
  os.makedirs("Articles",exist_ok=True)
  file_name = re.sub(r'[\\/:*?"<>|]', "_", f"{author}-{title}-{create_time}.html")
  html_path = os.path.join("HTML",file_name)
  print(f"HTMLé•¿åº¦: {len(html)}")
  print(f"æ–‡ç« å†…å®¹é•¿åº¦: {len(content)}")
  print("-----å¼€å§‹ä¿å­˜HTMLæºç åˆ°æœ¬åœ°-----")
  with open(html_path,"w",encoding="utf-8") as f:
    f.write(html)
  print("-----æˆåŠŸä¿å­˜HTMLæºç åˆ°æœ¬åœ°-----")
  file_name = re.sub(r'[\\/:*?"<>|]', "_", f"{author}-{title}-{create_time}.txt")
  txt_path = os.path.join("Articles",file_name)
  print("-----å¼€å§‹ä¿å­˜æ–‡ç« æ–‡æœ¬åˆ°æœ¬åœ°-----")
  with open(txt_path,"w",encoding="utf-8") as f:
    f.write(content)
  print("-----æˆåŠŸä¿å­˜æ–‡ç« æ–‡æœ¬åˆ°æœ¬åœ°-----")
  return {"status":1,"content":content,"title":title,"author":author,"create_time":create_time,"biz":biz}

def fetch_article_list(url,timeout):
  """
  åŠŸèƒ½ï¼šé€šè¿‡å®Œæ•´çš„å…¬ä¼—å·ä¸»é¡µgetmsgURLè·å–æ–‡ç« åˆ—è¡¨
  :param url:å®Œæ•´URL,
  :param timeout:è·å–é—´éš”,
  :return:Jsonæ•°æ®,åŸå§‹è¿”å›æ•°æ®
  """
  resp = s.get(url,timeout=timeout)
  try:
    data = resp.json()
    return data,resp
  except Exception:
    print("æ²¡æœ‰è·å–åˆ°æ–‡ç« ï¼")
    return None,resp

